{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Character Embedding Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5XRNFjqXeqH",
        "colab_type": "text"
      },
      "source": [
        "Here's the data setup from [Project 0](https://cal-cs288.github.io/sp20/projects/proj0.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T1ijH6-WlSp",
        "colab_type": "code",
        "outputId": "daea3a66-1a97-48e9-b4c8-34fa68a476ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "brown_tokens = brown.tagged_words(tagset='universal')\n",
        "print('Tagged tokens example: ', brown_tokens[:5])\n",
        "print('Total # of word tokens:', len(brown_tokens))\n",
        "\n",
        "max_word_len = 20\n",
        "\n",
        "def most_common(s):\n",
        "    \"Return the most common element in a sequence.\"\n",
        "    return Counter(s).most_common(1)[0][0]\n",
        "\n",
        "def most_common_tags(tagged_words, min_count=3, max_len=max_word_len):\n",
        "    \"Return a dictionary of the most common tag for each word, filtering a bit.\"\n",
        "    counts = defaultdict(list)\n",
        "    for w, t in tagged_words:\n",
        "        counts[w.lower()].append(t)\n",
        "    return {w: most_common(tags) for w, tags in counts.items() if \n",
        "            w.isalpha() and len(w) <= max_len and len(tags) >= min_count}\n",
        "\n",
        "brown_types = most_common_tags(brown_tokens)\n",
        "print('Tagged types example: ', sorted(brown_types.items())[:5])\n",
        "print('Total # of word types:', len(brown_types))\n",
        "\n",
        "def split(items, test_size):\n",
        "    \"Randomly split into train, validation, and test sets with a fixed seed.\"\n",
        "    random.Random(288).shuffle(items)\n",
        "    once, twice = test_size, 2 * test_size\n",
        "    return items[:-twice], items[-twice:-once], items[-once:]\n",
        "\n",
        "val_test_size = 1000\n",
        "all_data_raw = split(sorted(brown_types.items()), val_test_size)\n",
        "train_data_raw, validation_data_raw, test_data_raw = all_data_raw\n",
        "all_tags = sorted(set(brown_types.values()))\n",
        "print('Tag options:', all_tags)\n",
        "\n",
        "##\n",
        "\n",
        "def noun_predictor(raw_data):\n",
        "    \"A predictor that always predicts NOUN.\"\n",
        "    predictions = []\n",
        "    for word, _ in raw_data:\n",
        "        predictions.append('NOUN')\n",
        "    return predictions\n",
        "\n",
        "def accuracy(predictions, targets):\n",
        "    \"\"\"Return the accuracy percentage of a list of predictions.\n",
        "    \n",
        "    predictions has only the predicted tags\n",
        "    targets has tuples of (word, tag)\n",
        "    \"\"\"\n",
        "    assert len(predictions) == len(targets)\n",
        "    n_correct = 0\n",
        "    for predicted_tag, (word, gold_tag) in zip(predictions, targets):\n",
        "        if predicted_tag == gold_tag:\n",
        "            n_correct += 1\n",
        "\n",
        "    return n_correct / len(targets) * 100.0\n",
        "\n",
        "def evaluate(predictor, raw_data):\n",
        "    return accuracy(predictor(raw_data), raw_data)\n",
        "\n",
        "print('noun baseline validation accuracy:', \n",
        "      evaluate(noun_predictor, validation_data_raw))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "Tagged tokens example:  [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN')]\n",
            "Total # of word tokens: 1161192\n",
            "Tagged types example:  [('a', 'DET'), ('aaron', 'NOUN'), ('ab', 'NOUN'), ('abandon', 'VERB'), ('abandoned', 'VERB')]\n",
            "Total # of word types: 18954\n",
            "Tag options: ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n",
            "noun baseline validation accuracy: 55.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiZ4q4aqVRVF",
        "colab_type": "text"
      },
      "source": [
        "The last three letters of a word carry information about the most common part of speech. Here's how you might exploit that fact without a neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOUxqgvkWcwt",
        "colab_type": "code",
        "outputId": "cef165aa-dc7f-44a1-a755-46a0420b123a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from itertools import groupby\n",
        "\n",
        "# Find all the ends of words in the training set.\n",
        "last_letters = {w[-3:] for w, _ in train_data_raw}\n",
        "\n",
        "# For each word ending, find the most common part of speech.\n",
        "last_letter_tags = {c: most_common([t for _, t in words]) for \n",
        "                    c, words in groupby(train_data_raw, key=lambda x: x[0][-3:])}\n",
        "\n",
        "def last_letter_predictor(raw_data):\n",
        "    \"Guess the most common part of speech based only on the last 3 letters.\"\n",
        "    predictions = []\n",
        "    for word, _ in raw_data:\n",
        "        # If the end of this word hasn't been seen before, guess 'NOUN'\n",
        "        predictions.append(last_letter_tags.get(word[-3:], 'NOUN'))\n",
        "    return predictions\n",
        "\n",
        "print('last letter validation accuracy:', \n",
        "      evaluate(last_letter_predictor, validation_data_raw))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last letter validation accuracy: 76.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ramiVKNdQHf",
        "colab_type": "text"
      },
      "source": [
        "To achieve a similar result with a neural network, we first create data matrices that contain only the last three letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxL4QMQ0dOBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_matrices(data_raw):\n",
        "    \"\"\"Convert a list of (word, tag) pairs into tensors with appropriate padding.\n",
        "    \n",
        "    character_matrix holds character codes for each word, \n",
        "      indexed as [word_index, character_index]\n",
        "    character_mask masks valid characters (1 for valid, 0 invalid), \n",
        "      indexed similarly so that all inputs can have a constant length\n",
        "    pos_labels holds part-of-speech one-hot vectors,\n",
        "      indexed as [word_index, pos_index] with 0/1 values\n",
        "    \"\"\"\n",
        "    # This part is slightly different from the project because we only\n",
        "    # allocate space for three letters instead of all of them.\n",
        "    character_matrix = torch.zeros(len(data_raw), 3, dtype=torch.int64) \n",
        "    character_mask = torch.zeros(len(data_raw), 3, dtype=torch.float32)\n",
        "\n",
        "    pos_labels = torch.zeros(len(data_raw), dtype=torch.int64)\n",
        "    \n",
        "    for word_i, (word, pos) in enumerate(data_raw):\n",
        "        # This part is different from the project. Instead of encoding all\n",
        "        # letters, we just encode the last three.\n",
        "        last_three_letters = word[:-3:-1]  # reversed, in case of 2-letter words\n",
        "                                           # E.g., 'dogs' => 'sgo'\n",
        "        for char_i, c in enumerate(last_three_letters):\n",
        "            character_matrix[word_i, char_i] = ord(c)\n",
        "\n",
        "            # The mask is only 1 if there is a letter in the char_i position.\n",
        "            # For 1-letter and 2-letter words, the character_mask would have 0's.\n",
        "            character_mask[word_i, char_i] = 1\n",
        "        pos_labels[word_i] = all_tags.index(pos)\n",
        "    return torch.utils.data.TensorDataset(character_matrix, character_mask, pos_labels)\n",
        "\n",
        "validation_data = make_matrices(validation_data_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIxSaG7pavpS",
        "colab_type": "text"
      },
      "source": [
        "Here's an illustrated network that makes a prediction based on these three letters. This network is not identical to the lookup-based approach above, but it produces a similar validation accuracy (around 76%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N8wiRpoiZzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LastLettersNetwork(nn.Module):\n",
        "    def __init__(self, n_outputs):\n",
        "        super().__init__()\n",
        "\n",
        "        char_types = 128  # An upper bound on the number of unique characters\n",
        "        # we actually will only use a smaller set of characters, but we are using ascii codes for the character ids that can have values up to 127\n",
        "\n",
        "        embedding_size = 32  # Arbitrary dimensionality of the embeddings\n",
        "\n",
        "        # Create a parameter for the embedding vectors for each character.\n",
        "        # An embedding is a learned position in some vector space that has some\n",
        "        # arbitrary dimension, such as 32 (small) or 128 (more typical).\n",
        "        self.emb = nn.Embedding(char_types, embedding_size)\n",
        "\n",
        "        # Create a projection layer to project three stacked character embedding \n",
        "        # vectors into the output space.\n",
        "        # The second size n_outputs is the number of possible parts of speech.\n",
        "        self.output_layer = nn.Linear(3 * embedding_size, n_outputs)\n",
        "\n",
        "    def forward(self, chars, mask):\n",
        "        # Embed all three characters. Even for 2-letter words, we embed all 3.\n",
        "        # Now x is a tensor for a batch of words with dimensions that tell you\n",
        "        # (which example in batch, which character, which element of the embedding)\n",
        "        x = self.emb(chars)\n",
        "\n",
        "        # Mask out any characters that aren't there (for 1- and 2-letter words).\n",
        "        # The \"unsqueeze\" call is needed to edit the shape of the mask tensor,\n",
        "        # so that it is compatible with the shape of the embedding tensor.\n",
        "        #\n",
        "        # The dimensions of these things are...\n",
        "        # x:                 (# examples, # characters, hidden_size)\n",
        "        # mask:              (# examples, # characters)\n",
        "        # mask.unsqueeze(2): (# examples, # characters, 1)\n",
        "        #\n",
        "        # So, x and mask.unsqueeze(2) aren't the same dimension, but they\n",
        "        # are compatible under broadcasting: \n",
        "        # https://pytorch.org/docs/stable/notes/broadcasting.html\n",
        "        x = x * mask.unsqueeze(2)\n",
        "\n",
        "        # For each element in the batch, there's a (3, hidden_size) matrix.\n",
        "        # Instead, build a (hidden_size * 3) vector as a concatenation of all\n",
        "        # three character vectors. torch.reshape could achieve this too,\n",
        "        # but torch.cat is handy for the last part of the project. \n",
        "        x = torch.cat((x[:,0,:], x[:,1,:], x[:,2,:]), dim=1)\n",
        "\n",
        "        # It would be reasonable to add more layers, non-linearity and dropout.\n",
        "\n",
        "        # Project into the output space.\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}